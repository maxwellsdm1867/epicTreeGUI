---
phase: 00-testing-validation
plan: 05
type: execute
wave: 3
depends_on: ["00-02", "00-03", "00-04"]
files_modified:
  - tests/integration/WorkflowTest.m
  - TESTING_REPORT.md
autonomous: true

must_haves:
  truths:
    - "Complete workflow from data loading through tree building to analysis produces valid results"
    - "Workflow with selection filtering produces different results than unfiltered"
    - "TESTING_REPORT.md captures all bugs, performance issues, and design inconsistencies found"
    - "All test suites pass when run together as a full test run"
  artifacts:
    - path: "tests/integration/WorkflowTest.m"
      provides: "End-to-end workflow integration tests"
      contains: "classdef WorkflowTest"
    - path: "TESTING_REPORT.md"
      provides: "Complete testing report with all findings"
      contains: "Testing Report"
  key_links:
    - from: "tests/integration/WorkflowTest.m"
      to: "src/loadEpicTreeData.m"
      via: "Data loading entry point"
      pattern: "loadEpicTreeData"
    - from: "tests/integration/WorkflowTest.m"
      to: "src/tree/epicTreeTools.m"
      via: "Tree building"
      pattern: "epicTreeTools"
    - from: "tests/integration/WorkflowTest.m"
      to: "src/analysis/getMeanResponseTrace.m"
      via: "Analysis function calls in workflow"
      pattern: "getMeanResponseTrace"
    - from: "tests/integration/WorkflowTest.m"
      to: "src/getSelectedData.m"
      via: "Data extraction in workflow"
      pattern: "getSelectedData"
---

<objective>
Write end-to-end integration tests that validate complete analysis workflows, run the full test suite, and finalize the TESTING_REPORT.md with all findings from Phase 0.

Purpose: Unit tests validate individual functions in isolation. Integration tests validate that functions work together in realistic workflows -- the same sequences a researcher would actually use. This is the final validation gate before proceeding to documentation phases. The testing report captures everything discovered during Phase 0 as a reference for future development.

Output: WorkflowTest class with end-to-end tests, finalized TESTING_REPORT.md, and a full passing test suite.
</objective>

<execution_context>
@/Users/maxwellsdm/.claude/get-shit-done/workflows/execute-plan.md
@/Users/maxwellsdm/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/00-testing-validation/00-CONTEXT.md

# Reference all prior plan summaries for accumulated bugs/issues
@.planning/phases/00-testing-validation/00-01-SUMMARY.md
@.planning/phases/00-testing-validation/00-02-SUMMARY.md
@.planning/phases/00-testing-validation/00-03-SUMMARY.md
@.planning/phases/00-testing-validation/00-04-SUMMARY.md

# Source files used in workflows
@src/loadEpicTreeData.m
@src/tree/epicTreeTools.m
@src/getSelectedData.m
@src/analysis/getMeanResponseTrace.m
@src/analysis/getResponseAmplitudeStats.m

# Test helpers
@tests/helpers/loadTestTree.m
@TESTING_REPORT.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write end-to-end WorkflowTest class</name>
  <files>
    tests/integration/WorkflowTest.m
  </files>
  <action>
Create `tests/integration/WorkflowTest.m` inheriting from `matlab.unittest.TestCase`. These tests replicate complete researcher workflows from data loading through analysis.

Test class structure:
```
classdef WorkflowTest < matlab.unittest.TestCase
    properties
        DataPath
        H5Dir
    end
    methods (TestClassSetup)
        function setupPaths(testCase)
            addpath(genpath('src'));
            addpath(genpath('tests/helpers'));
            testCase.DataPath = getTestDataPath();
        end
    end
end
```

Required test methods:

**Workflow 1: Basic Analysis Pipeline**
- `testLoadBuildAnalyzeWorkflow` - Complete pipeline:
  1. Load data with loadEpicTreeData()
  2. Build tree with epicTreeTools and splitOnCellType
  3. Navigate to first cell type child
  4. Call getMeanResponseTrace on that node
  5. Verify output struct has valid fields
  6. This is THE critical path -- if this fails, the tool is broken

**Workflow 2: Multi-Level Tree Navigation + Analysis**
- `testMultiLevelNavigationWorkflow` - Pipeline with deeper tree:
  1. Build tree with two split levels: cellInfo.type, blockInfo.protocol_name
  2. Navigate: root -> first cell type -> first protocol
  3. Extract data with getSelectedData
  4. Compute amplitude stats with getResponseAmplitudeStats
  5. Store results with putCustom
  6. Verify stored results can be retrieved with getCustom

**Workflow 3: Selection-Filtered Analysis**
- `testSelectionFilteredWorkflow` - Verify selection filtering works end-to-end:
  1. Build tree
  2. Get a leaf node, note its epochCount
  3. Deselect half the epochs
  4. Call getMeanResponseTrace (or getSelectedData)
  5. Verify n in output equals the selected count (not total count)
  6. This validates that selection state flows through the entire pipeline

**Workflow 4: Comparative Analysis Across Conditions**
- `testComparativeAnalysisWorkflow` - Multiple condition comparison:
  1. Build tree with cellInfo.type
  2. Get all cell type children
  3. For each: compute getMeanResponseTrace
  4. Compare n values across conditions (should differ or be same depending on data)
  5. Store results at each node
  6. Query results back from all leaf nodes

**Workflow 5: Tree Reorganization**
- `testTreeReorgWorkflow` - Verify tree rebuilding preserves data:
  1. Build tree with cellInfo.type, count total epochs
  2. Rebuild with blockInfo.protocol_name
  3. Count total epochs again
  4. Verify counts match (no data lost during reorganization)
  5. Verify tree structure changed (different split values at level 1)

**Workflow 6: GUI + Analysis Integration (if GUI available)**
- `testGUIAnalysisWorkflow` - Full GUI pipeline:
  1. Build tree
  2. Launch GUI with epicTreeGUI(tree)
  3. Use getSelectedEpochTreeNodes() to get root
  4. Navigate programmatically to a leaf
  5. Extract data
  6. Close GUI
  7. Use `testCase.addTeardown(@() close(gui.figure))`

Implementation notes:
- Each test is independent -- loads data fresh
- Use loadTestTree helper where appropriate, but Workflow 1 should show the raw loading path
- Close any figures opened during tests
- Workflow 6 (GUI) may be slow -- consider using TestClassSetup to share GUI instance with careful state reset
- Fix any bugs found and document in TESTING_REPORT.md
  </action>
  <verify>
    Run via MCP MATLAB tools:
    - `results = runtests('tests/integration/WorkflowTest');`
    - All tests pass (0 failures, 0 errors)
    - Each workflow produces valid output end-to-end
  </verify>
  <done>
    WorkflowTest validates 6 complete researcher workflows: basic analysis, multi-level navigation, selection filtering, comparative analysis, tree reorganization, and GUI integration. All workflows produce correct results end-to-end. Zero test failures.
  </done>
</task>

<task type="auto">
  <name>Task 2: Run full test suite and finalize TESTING_REPORT.md</name>
  <files>
    TESTING_REPORT.md
  </files>
  <action>
Run the complete test suite across all test classes and finalize the testing report.

1. Run full test suite via MCP MATLAB tools:
```matlab
results = runtests({'tests/unit', 'tests/gui', 'tests/integration'});
disp(results);
```

2. If any tests fail, diagnose and fix:
   - Read the failure message
   - Determine if it's a test issue or source code bug
   - Fix the root cause
   - Document in TESTING_REPORT.md
   - Re-run until all pass

3. Finalize TESTING_REPORT.md with:

**Summary section:**
- Total test count (unit + GUI + integration)
- Pass/fail/skip counts
- Date of test run
- MATLAB version

**Bugs Found table:**
- Compile all bugs found across Plans 01-05
- Each with: ID (BUG-001, BUG-002...), Description, Severity (Critical/Major/Minor), Status (Fixed/Open), Fix Commit hash
- If no bugs found, state "No bugs found"

**Performance Issues table:**
- Any slow operations noticed during testing
- Memory-intensive operations
- If none, state "No performance issues identified"

**Design Inconsistencies table:**
- API inconsistencies (different function signatures for similar operations)
- Naming mismatches (e.g., 'parameters' vs 'protocolSettings')
- Pattern violations
- If none, state "No design inconsistencies identified"

**Test Coverage Summary:**
- Table showing: Category | Test Class | Test Count | Status
- Categories: Tree Navigation, Splitter Functions, Data Extraction, Analysis Functions, GUI Interaction, Integration Workflows
- All should show "All Pass" status

**Baseline Status:**
- List all golden baseline files
- Note which analysis functions have baselines and which were skipped

4. Ensure TESTING_REPORT.md is comprehensive enough to serve as Phase 0's primary deliverable.
  </action>
  <verify>
    Run via MCP MATLAB tools:
    - `results = runtests({'tests/unit', 'tests/gui', 'tests/integration'});`
    - `disp(table(results));` shows all passed
    - Zero failures across entire test suite
    - TESTING_REPORT.md exists with complete content
  </verify>
  <done>
    Full test suite passes with zero failures. TESTING_REPORT.md is complete with: summary, bugs found (with fix status), performance issues, design inconsistencies, test coverage summary, and baseline status. Phase 0 testing is complete and documented.
  </done>
</task>

</tasks>

<verification>
1. `results = runtests({'tests/unit', 'tests/gui', 'tests/integration'})` - all pass
2. TESTING_REPORT.md has: Summary, Bugs Found, Performance Issues, Design Inconsistencies, Test Coverage Summary, Baseline Status
3. All bugs discovered during Phase 0 are either fixed or documented as open
4. End-to-end workflows validate the complete researcher experience
5. No test failures across the entire suite
</verification>

<success_criteria>
- Full test suite (unit + GUI + integration) passes with zero failures
- 6 end-to-end workflows validated successfully
- TESTING_REPORT.md is comprehensive and finalized
- All bugs found during Phase 0 are fixed and documented with commit references
- Performance issues and design inconsistencies identified and documented
- Phase 0 provides confidence that the tool works correctly with real data
</success_criteria>

<output>
After completion, create `.planning/phases/00-testing-validation/00-05-SUMMARY.md`
</output>
